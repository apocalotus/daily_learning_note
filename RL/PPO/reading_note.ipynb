{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Blog: 深度解读 Policy Gradient, PPO & PPG\n",
    "\n",
    "linnk: https://zhuanlan.zhihu.com/p/342150033\n",
    "\n",
    "## Policy Gradient method\n",
    "\n",
    "## GAE (Generalized Advantage Estimation)\n",
    "\n",
    "$TD(\\lambda)$- like advantage estimation\n",
    "\n",
    "In practice, instead of using infinite-length horizon, sample a fixed length of nstep, program it in a recursive manner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "def compute_gae(\n",
    "    vpred, # \"value predictions\",\n",
    "    reward,\n",
    "    first, # \"mark beginning of episodes\"\n",
    "    gamma,\n",
    "    lambda_gae\n",
    "):\n",
    "    orig_device = vpred.device\n",
    "    assert orig_device == reward.device == first.device\n",
    "    vpred, reward, first = (x.to('cpu') for x in (vpred, reward, first))\n",
    "    assert first.dim() == 2\n",
    "    nenv, nstep = reward.shape\n",
    "    assert vpred.shape == first.shape == (nenv, nstep + 1)\n",
    "    adv = torch.zeros([nenv, nstep], dtype=torch.float32)\n",
    "    lastgaelam = 0\n",
    "    \n",
    "    for t in reversed(range(nstep)):\n",
    "        notlast = 1 - first[:,t+1]\n",
    "        nextvalue = vpred[:t+1]\n",
    "        delta = reward[:,t] + gamma * nextvalue - vpred[:,t]\n",
    "        adv[:,t] = lastgaelam = delta + notlast * gamma * lambda_gae * lastgaelam\n",
    "\n",
    "    vtarg = vpred[:,:-1] + adv\n",
    "    return adv.to(orig_device), vtarg.to(orig_device)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Here we need have an extra step for value(vpred) and terminal(first) to calculate the desired gae and value target."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculation of log likelihood\n",
    "\n",
    "For discrete action space, it is a Catergorical Distribution.\n",
    "\n",
    "For continuous action space, it is usually a Gaussian Distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gumbel max sampling in openai baseline\n",
    "import tensorflow as tf\n",
    "def sample(self):\n",
    "    u = tf.random_uniform(tf.shape(self.logits), dtype=self.logits.dtype)\n",
    "    return tf.argmax(self.logits - tf.log(-tf.log(u)), axis=-1)\n",
    "\n",
    "# equivalent to multinomial sampling yet more efficient"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gumbel Distribution\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/gumbel1.png  width=50%/>\n",
    "\n",
    "<div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">Gumbel Distribution</div>\n",
    "\n",
    "<br>\n",
    "\n",
    "the Gumbel distribution is typically used to model the maximum of a set of independent samples, and it should be a good fit when the underlying data is distributed according to either a normal or an exponential distribution. Furthermore, the multinomial random choice of a discrete distribution can be viewed as the operation of argmax on original logit plus a gumbel noise.\n",
    "\n",
    "The Gumbel-Max Trick for Discrete Distributions.\n",
    "\n",
    "link: https://hips.seas.harvard.edu/blog/2013/04/06/the-gumbel-max-trick-for-discrete-distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Manually generating Gumbel Distribution from uniform distribution\n",
    "from matplotlib.pyplot import *\n",
    "import numpy as np\n",
    "from numpy import log\n",
    "n_samples = 1000\n",
    "numpy_gumbel = np.random.gumbel(size=n_samples)\n",
    "manual_gumbel = -log(-log(np.random.uniform(size=n_samples)))\n",
    "figure()\n",
    "subplot(1, 2, 1)\n",
    "hist(numpy_gumbel, bins=50)\n",
    "ylabel(\"Probability\")\n",
    "xlabel(\"numpy Gumbel\")\n",
    "subplot(1, 2, 2)\n",
    "hist(manual_gumbel, bins=50)\n",
    "xlabel(\"Gumbel from uniform noise\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Gumbel-softmax trick\n",
    "Remember, the Gumbel-max trick produces a sample by adding noise to the logits then taking the argmax of the resulting vector. So far, there's nothing so special about Gumbel-max sampling.\n",
    "\n",
    "But what if we approximate the argmax by a (low-temperature) softmax? Then something really interesting happens: we have a chain of operations that's fully differentiable. We have differentiable sampling operator (albeit with a soft one-hot output instead of a scalar). Wow!\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/differentiable_argmax.webp  width=60%/>\n",
    "\n",
    "<div style=\"color:orange; border-bottom: 1px solid #d9d9d9;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">Differentiable argmax in a NN</div>\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## PPO\n",
    "\n",
    "PPO is an improvement on  vanilla pocily gradient, inheriting from TRPO.\n",
    "\n",
    "The biggest problem of PG methods is the bias of data distribution, deriving from a biased estimate of Advantage function. Therefore, if an update of policy is too far off, the next sampling will be dramatically off road, inducing a malicious cycle. TRPO addresses this problem by keeping policy updates in a trust region.\n",
    "\n",
    "### Use of importance sampling\n",
    "\n",
    "PG mthods are on policy, therefore sampling inefficient. For TRPO and PPO, they are mostly off-policy, or near on-policy due to sanple reuse.\n",
    "\n",
    "For PPO, usually we have sample_reuse = 3. (one on-policy update, the others are off-policy)\n",
    "\n",
    "Hence, we use IS to adjust.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/is.png  width=60%/>\n",
    "\n",
    "<div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">Importance Sampling</div>\n",
    "\n",
    "<br></div>\n",
    "\n",
    "### Clipped Surrogate Objective\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/clip.webp width=80%/>\n",
    "\n",
    "<div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">clip-PPO</div>\n",
    "</div>\n",
    "\n",
    "The idea of PPO is  that we clip the ratio of current and old policies, if the ratio exceeds $\\epsilon$, it will be clipped, and the gradient will be zero, stopping further deviation"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Training Flow\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/algorithm.webp width=60%/>\n",
    "\n",
    "<div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">PPO</div>\n",
    "</div>\n",
    "\n",
    "Common setting: K = 3, minibatch_size = NT/4. That means 12 updates per sampling.\n",
    "\n",
    "### OpenAI Dota Five\n",
    "\n",
    "$$Sample Reuse := \\frac{(sample \\space per \\space batch) \\times (batches \\space per \\space second)}{(experience \\space buffer \\space intake \\space samples \\space per \\space second)}$$\n",
    "\n",
    "Sample reuse means sample use by learner per second divided by sample generation per second, thus can be smaller than 1.\n",
    "\n",
    "Experiments show that lower sample reuse imporve model perfermance, this is due to the huge batchsize and up to 32 updates in a single iteration, which is more than sufficient to update value function, unlike in small-scale scenario. In the practice by OpenAI, sample reuse is  0.8-2.7, and sample rerun is 1.0-1.1, implying avoiding sample rerun.\n",
    "\n",
    "<div align=\"center\">\n",
    "<img src=asset/openai_five.webp width=80%/>\n",
    "\n",
    "<div style=\"color:orange;\n",
    "    display: inline-block;\n",
    "    color: #999;\n",
    "    padding: 2px;\">OpenAI Five</div>\n",
    "</div>\n",
    "\n",
    "\n",
    "### PPO Loss\n",
    "\n",
    "PPO loss contains three parts: policy loss, value funtion loss and entropy loss.\n",
    "\n",
    "Common setting: $c_2 = 0.01 \\rightarrow 0.001, \\space learning rate = 5\\times 10^{-5} \\rightarrow 5 \\times 10^{-6}$\n",
    "\n",
    "### Clipping and Normalization for Obs, Reward and Advantage \n",
    "\n",
    "To make PPO work for differnet game scenario, clipping and normalization are key.\n",
    "\n",
    "Usually, we do clipping after noramlization.\n",
    "\n",
    "Advantage normalization is direct, and can be operated on batch data, assuming that advantage has stable distribution.\n",
    "\n",
    "For rewards, we use running avg.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RewardNoramlizer:\n",
    "    def __init__(self, num_envs, cliprew=10.0, gamma=0.99, epsilon=1e-8, per_env=False) -> None:\n",
    "        ret_rms_shape =(num_env,) if per_env else ()\n",
    "        self.ret_rms = RunningMeanStd(shape=ret_rms_shape)\n",
    "        self.cliprew = cliprew\n",
    "        self.ret = th.zeros(num_envs)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.per_env = per_env\n",
    "    \n",
    "    def __call__(self, reward, first):\n",
    "        rets = backward_discounted_sum(\n",
    "            prevret=self.ret, reward=reward.cpu(), fisrt=first.cpu(), gamma=self.gamma\n",
    "        )\n",
    "        self.ret = ret[:,-1]\n",
    "        self.ret_rms.update(rets if self.per_env else rets.reshape(-1))\n",
    "        return self.transform(reward)\n",
    "    \n",
    "    def transform(self, reward):\n",
    "        return th.clamp(\n",
    "            reward / th.sqrt(self.ret_rms.var + self.epsilon),\n",
    "            -self.cliprew,\n",
    "            self.cliprew,\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# observation normalization\n",
    "\n",
    "def _normalize_clip_observation(x, clip_range=[-5.0, 5.0]):\n",
    "    rms = RunningMeanStd(shape=x.shape[1:])\n",
    "    norm_x = tf.clip_by_value((x - rms.mean) / rms.std, min(clip_range), max(clip_range))\n",
    "    return norm_x, rms"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**\n",
    "\n",
    "Recommendation: Always use observation normalization and check if value function normalization improves performance. Gradient clipping might slightly help but is of secondary importance."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Network Initialization\n",
    "\n",
    "In PPG, the neural network uses Normalized Fan-in init(std is calculated using the norm of fan-in weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def NormedLinear(*args, scale=1.0, dtype=th.float32, **kwargs):\n",
    "    \"\"\"\n",
    "    nn.Linear but with normalized fan-in init\n",
    "    \"\"\"\n",
    "    dtype = parse_dtype(dtype)\n",
    "    if dtype == th.float32:\n",
    "        out = nn.Linear(*args, **kwargs)\n",
    "    elif dtype == th.float16:\n",
    "        out = LinearF16(*args, **kwargs)\n",
    "    else:\n",
    "        raise ValueError(dtype)\n",
    "    out.weight.data *= scale / out.weight.norm(dim=1, p=2, keepdim=True)\n",
    "    if kwargs.get(\"bias\", True):\n",
    "        out.bias.data *= 0\n",
    "    return out\n",
    "\n",
    "def NormedConv2d(*args, scale=1, **kwargs):\n",
    "    \"\"\"\n",
    "    nn.Conv2d but with normalized fan-in init\n",
    "    \"\"\"\n",
    "    out = nn.Conv2d(*args, **kwargs)\n",
    "    out.weight.data *= scale / out.weight.norm(dim=(1, 2, 3), p=2, keepdim=True)\n",
    "    if kwargs.get(\"bias\", True):\n",
    "        out.bias.data *= 0\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def kaiming_normal_(tensor, a=0, mode='fan_in', nonlinearity='leaky_relu'):\n",
    "    fan = _calculate_correct_fan(tensor, mode) \n",
    "    # 通过mode判断是前向传播还是反向传播, 生成不同的一个fan值.\n",
    "    gain = calculate_gain(nonlinearity, a)\n",
    "    # 通过判断是哪种激活函数生成一个gain值\n",
    "    std = gain / math.sqrt(fan) # 通过fan值和gain值进行标准差的计算\n",
    "    with torch.no_grad():\n",
    "        return tensor.normal_(0, std)\n",
    "\n",
    "def _calculate_fan_in_and_fan_out(tensor):\n",
    "    dimensions = tensor.dim() # 返回的是维度\n",
    "    if dimensions < 2:\n",
    "        raise ValueError(\"Fan in and fan out can not be computed for tensor with fewer than 2 dimensions\")\n",
    "    if dimensions == 2:  # Linear\n",
    "        fan_in = tensor.size(1) \n",
    "        fan_out = tensor.size(0)\n",
    "    else:\n",
    "        num_input_fmaps = tensor.size(1) # 卷积的输入通道大小\n",
    "        num_output_fmaps = tensor.size(0) # 卷积的输出通道大小\n",
    "        receptive_field_size = 1\n",
    "        if tensor.dim() > 2:\n",
    "            receptive_field_size = tensor[0][0].numel() # 卷积核的大小:k*k\n",
    "        fan_in = num_input_fmaps * receptive_field_size # 输入通道数量*卷积核的大小. 用于前向传播\n",
    "        fan_out = num_output_fmaps * receptive_field_size # 输出通道数量*卷积核的大小. 用于反向传播\n",
    "\n",
    "    return fan_in, fan_out\n",
    "\n",
    "def _calculate_correct_fan(tensor, mode):\n",
    "    mode = mode.lower()\n",
    "    valid_modes = ['fan_in', 'fan_out']\n",
    "    if mode not in valid_modes:\n",
    "        raise ValueError(\"Mode {} not supported, please use one of {}\".format(mode, valid_modes))\n",
    "\n",
    "    fan_in, fan_out = _calculate_fan_in_and_fan_out(tensor)\n",
    "    return fan_in if mode == 'fan_in' else fan_out\n",
    "\n",
    "def calculate_gain(nonlinearity, param=None):\n",
    "    r\"\"\"Return the recommended gain value for the given nonlinearity function.\n",
    "    The values are as follows:\n",
    "\n",
    "    ================= ====================================================\n",
    "    nonlinearity      gain\n",
    "    ================= ====================================================\n",
    "    Linear / Identity :math:`1`\n",
    "    Conv{1,2,3}D      :math:`1`\n",
    "    Sigmoid           :math:`1`\n",
    "    Tanh              :math:`\\frac{5}{3}`\n",
    "    ReLU              :math:`\\sqrt{2}`\n",
    "    Leaky Relu        :math:`\\sqrt{\\frac{2}{1 + \\text{negative\\_slope}^2}}`\n",
    "    ================= ====================================================\n",
    "\n",
    "    Args:\n",
    "        nonlinearity: the non-linear function (`nn.functional` name)\n",
    "        param: optional parameter for the non-linear function\n",
    "\n",
    "    Examples:\n",
    "        >>> gain = nn.init.calculate_gain('leaky_relu', 0.2)  # leaky_relu with negative_slope=0.2\n",
    "    \"\"\"\n",
    "    linear_fns = ['linear', 'conv1d', 'conv2d', 'conv3d', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d']\n",
    "    if nonlinearity in linear_fns or nonlinearity == 'sigmoid':\n",
    "        return 1\n",
    "    elif nonlinearity == 'tanh':\n",
    "        return 5.0 / 3\n",
    "    elif nonlinearity == 'relu':\n",
    "        return math.sqrt(2.0)\n",
    "    elif nonlinearity == 'leaky_relu':\n",
    "        if param is None:\n",
    "            negative_slope = 0.01\n",
    "        elif not isinstance(param, bool) and isinstance(param, int) or isinstance(param, float):\n",
    "            # True/False are instances of int, hence check above\n",
    "            negative_slope = param\n",
    "        else:\n",
    "            raise ValueError(\"negative_slope {} not a valid number\".format(param))\n",
    "        return math.sqrt(2.0 / (1 + negative_slope ** 2))\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported nonlinearity {}\".format(nonlinearity))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialize the last policy layer with 100x smaller weights. Use softplus function $ln(1 + e^x)$ to transform network output into action and add a negative offset to its input to decrease the initial std of actions. Tune this offset if possible. Use tanh both as the activation function (if the networks are not too deep) and to transform the samples from the normal distribution to the bounded action space. Use a wide value MLP (no layers shared with the policy) but tune the policy width(it might need to be narrower than the calue MLP)\n",
    "\n",
    "初始化最后策略层的权重：建议在初始化策略网络的最后一层时，使用比正常小100倍的权重。这样做的目的可能是为了减缓学习过程的开始阶段，防止在训练初期出现过大的动作（actions）输出，从而有助于稳定训练过程。\n",
    "\n",
    "使用softplus函数转换网络输出为动作的标准差：在强化学习中，动作的不确定性是一个重要的因素。softplus函数（σ(x) = ln(1 + e^x)）是一个平滑且单调递增的函数，它能够将网络的输出转换为一个正的标准差值，这对于实现动作的不确定性模型（如高斯策略）非常有用。\n",
    "\n",
    "添加负偏移以减小初始动作的标准差：在softplus函数之前添加一个负偏移量，可以进一步减小初始动作的标准差。这样做可以确保在训练开始时，策略不会生成过大的动作，有助于避免潜在的不稳定性。\n",
    "\n",
    "调整偏移量：如果可能的话，应该调整这个偏移量以优化性能。这可能涉及到超参数调优的过程，通过实验找到最佳的偏移量。\n",
    "\n",
    "使用tanh作为激活函数：如果网络不是太深，可以使用tanh（双曲正切函数）作为激活函数。tanh函数的输出范围是[-1, 1]，这使得它适合用于将网络输出规范化到一个有界的动作空间。\n",
    "\n",
    "使用宽度较大的值网络（Value MLP）：值网络（value network）用于估计状态的预期回报。建议使用一个宽度较大的多层感知机（MLP），但不与策略网络共享层。这样做可以提高值网络的表达能力，更准确地估计值函数。\n",
    "\n",
    "调整策略宽度：策略网络的宽度（即层的数量和每层的神经元数量）可能需要根据实际情况进行调整。有时，为了控制动作的方差，可能需要一个比值网络更窄的策略网络。"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Recommendation**\n",
    "\n",
    "Paper: https://arxiv.org/abs/2006.05990\n",
    "\n",
    "+ Policy losses: Use the PPO policy loss. Start with the clipping threshold set to 0.25, but also try lower and higher values if possible.\n",
    "+ Networks architecture: see above\n",
    "+ Normalization and clipping: Always use obs norm and check if value function norm improves performance. Gradient clipping might slightly help but is of secondary importance.\n",
    "+ Advantage Estimation: Use GAE with $\\lambda = 0.9$ but neither Huber loss(combining MSE and MAE) nor PPO-style value loss clipping for value net training.\n",
    "+ Training setup: Go over experience multiple times. Shuffle individual transitions before assigning them to minibatches and recompute advantages once per data pass. For faster wall-clock time training use many parallel environments and increase batch size(both might hurt the sample complexity). Tune the number of transitions in each iteration if possible.\n",
    "+ Timesteps handling: Discount factor $\\gamma$ is one of the most important hyperparameters and should be tuned per environment(start with $\\gamma = 0.99$). Try frame skip if possible. There is no need to handle environments step limits in a special way for large step limits.\n",
    "+ Use Adam optimizer with momentum $\\beta_1 = 0.9$ and a tuned learning rate(3e-4 is a safe default). Linearly decaying the learnign rate may slightly improve performance but is of secondary importance.\n",
    "+ Regularization: We do not find evidence that any of the investigated regularizers helps significantly on our environments with the exception of HalfCheetah on which all constraints (especially the entropy constraint) help . However, the performance boost is largely independent on the constraint threshold which suggests that the effect is caused by the initial high strength of the penalty (before it gets adjusted) and not by the desired constraint. While it is a bit surprising that regularization does not help at all (apart from HalfCheetah), we conjecture that regularization might be less important in our experiments because: (1) the PPO policy loss already enforces the trust region which makes KL penalties or constraints redundant; and (2) the careful policy initialization (See Sec. 3.2) is enough to guarantee good exploration and makes the entropy bonus or constraint redundant."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### PPG\n",
    "\n",
    "![pic](asset/ppg.webp)\n",
    "\n",
    "\n",
    "![pic](asset/ppg2.webp)\n",
    "\n",
    "Experiments from PPG show that value funtion should be trained more, and for policy training, sample reuse should be close to 1."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[3.7417],\n",
       "        [7.0711]], dtype=torch.float64)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "a = torch.tensor([[1,2,3],[3,4,5]], dtype=float)\n",
    "a.norm(dim=1, p=2,keepdim=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "probs = np.random.randint(low=1, high=20, size=n_cats)\n",
    "probs = probs / sum(probs)\n",
    "logits = np.log(probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([19, 17,  4,  6, 15, 14, 14])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "probs = np.random.randint(low=1, high=20, size=n_cats)\n",
    "probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "noise = np.random.uniform(size=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.83017322, 0.35003149, 0.75920581, 0.6238727 , 0.38383317,\n",
       "       0.23929903, 0.06951028])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "noise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e8d1ec50fd7cdf00e23e95138235c186cd194dab816568c511b1f672e5e9780d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
