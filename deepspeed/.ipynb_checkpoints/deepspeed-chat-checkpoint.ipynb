{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Step1: Supervised Finetuning\n",
    "Problems occuring: repeating content generation & inconsistency between perplexit(PPL) scores & generation capabilities\n",
    "\n",
    "PPL scores $p(sentence)^{-1/N}$\n",
    "\n",
    "serveral terms affect the generation behavior:\n",
    "\n",
    "+ weight decay: OPT(Open Pre-trained Transformer Language Models) are pretrained with weight decay, althought we should inherits this setting, it may not produce the desired model.\n",
    "+ dropout: enabled in both OPT and Fintuning training\n",
    "+ dataset: using more data usually provides better model quality. Yet if the sources of datasets are too different, it may hurt the performance. For our OPT-1.3B example, we use the following four datasets: *Dahoas/rm-static  Dahoas/full-hh-rlhf, Dahoas/synthetic-instruct-gpth-piarwise, yitingxie/rlhf-reward-datasets*\n",
    "+ training epochs: Normally, to avoid overfitting, we hcoose smaller training epochs. However similar to InstructGPT pointed, we found even though we got overfitting dut to longer training, it is still recommended to use longer training epochs to get better generation quality. Particularly for our OPT-1.3B model, we use 16 epochs even though 1 ro 2 epochs training can reach the same PPL score.\n",
    "\n",
    "# Step2: Reward Model Finetuning\n",
    "\n",
    " difference from SRT\n",
    "\n",
    "+ training datasets: RM requires both good response and bad responses.\n",
    "+ training loss: RM requires pair ranking loss as the optimizing objective.\n",
    "  \n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e8d1ec50fd7cdf00e23e95138235c186cd194dab816568c511b1f672e5e9780d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
